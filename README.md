# PandasWorkFlow
PyPandas and NumPy Handson


# Added PySpark Library for my local Anaconda Distribution. 
Steps to follow

1. Dowload Apache Spark from the official WebSite.
1. Downlod Hadoop winutils.exe
1. Extract Spark Files and copy the winutils.exe to the spark BIN directory.

Set the following environment variables



Name						Value
1. SPARK_HOME					D:\spark\spark-2.2.1-bin-hadoop2.7
1. HADOOP_HOME					D:\spark\spark-2.2.1-bin-hadoop2.7
1. PYSPARK_DRIVER_PYTHON		jupyter
1. PYSPARK_DRIVER_PYTHON_OPTS	notebook


You are all set use PySpark !!! 

